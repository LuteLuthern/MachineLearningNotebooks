{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying a web service hosted on NVIDIA Triton to Azure Kubernetes Service (AKS)\n",
        "This notebook shows the steps for deploying a service with [NVIDIA Triton Inferencing Server](https://developer.nvidia.com/nvidia-triton-inference-server): registering a model, creating an image, provisioning a cluster (one time action), and deploying a service to it. \n",
        "We then test and delete the service, image and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "print(azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get workspace\n",
        "Load existing workspace from the config file info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (azureml-core 1.13.0a0 (/home/ralf/.local/lib/python3.6/site-packages), Requirement.parse('azureml-core~=1.10.0'), {'azureml-telemetry'}).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (azureml-core 1.13.0a0 (/home/ralf/.local/lib/python3.6/site-packages), Requirement.parse('azureml-core~=1.10.0'), {'azureml-telemetry'}).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.13.0a0 (/home/ralf/.local/lib/python3.6/site-packages), Requirement.parse('azureml-core~=1.10.0')).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.13.0a0 (/home/ralf/.local/lib/python3.6/site-packages), Requirement.parse('azureml-core~=1.10.0')).\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.13.0a0 (/home/ralf/.local/lib/python3.6/site-packages), Requirement.parse('azureml-core~=1.10.0')).\ntriton2\nyifyu\nwestus\n5f08d643-1910-4a38-a7c7-84a39d4f42e0\n"
        }
      ],
      "source": [
        "from azureml.core.workspace import Workspace\n",
        "\n",
        "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"5f08d643-1910-4a38-a7c7-84a39d4f42e0\")\n",
        "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"yifyu\")\n",
        "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"triton2\")\n",
        "ws = Workspace.get(\n",
        "    subscription_id = subscription_id,\n",
        "    resource_group = resource_group,\n",
        "    name = workspace_name)\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download the model\n",
        "\n",
        "Prior to registering the model, you should have a model in one of the [supported formats](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html#framework-model-definition) by Triton. This cell will download a [pretrained ONNX densenet](https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx) model.\n",
        "\n",
        "** Note: ** If you have a previously-registered model, the file name needs to follow the [naming convention](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html?highlight=model%20onnx#framework-model-definition) or be specified using model configuration file below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import shutil\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "from io import BytesIO\n",
        "\n",
        "model_url = 'https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx'\n",
        "\n",
        "version = '1'\n",
        "model_dir = os.path.join('models', 'triton', 'densenet_onnx', version)\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "\n",
        "target_file = os.path.join(model_dir, 'model.onnx')\n",
        "\n",
        "if not os.path.exists(target_file):\n",
        "    response = requests.get(model_url)\n",
        "    open(target_file, 'wb').write(response.content)\n",
        "\n",
        "config_file = os.path.join('models', 'triton', 'densenet_onnx', 'config.pbtxt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add Model Configuration file\n",
        "\n",
        "Each model needs a [Model Configuration](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_configuration.html) that provides required and optional information about the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting models/triton/densenet_onnx/config.pbtxt\n"
        }
      ],
      "source": [
        "%%writefile $config_file\n",
        "name: \"densenet_onnx\"\n",
        "platform: \"onnxruntime_onnx\"\n",
        "max_batch_size: 0\n",
        "input [\n",
        "  {\n",
        "    name: \"data_0\"\n",
        "    data_type: TYPE_FP32\n",
        "    format: FORMAT_NCHW\n",
        "    dims: [ 3, 224, 224 ]\n",
        "    reshape { shape: [ 1, 3, 224, 224 ] }\n",
        "  }\n",
        "]\n",
        "output [\n",
        "  {\n",
        "    name: \"fc6_1\"\n",
        "    data_type: TYPE_FP32\n",
        "    dims: [ 1000 ]\n",
        "    reshape { shape: [ 1, 1000, 1, 1 ] }\n",
        "    label_filename: \"densenet_labels.txt\"\n",
        "  }\n",
        "]\n",
        "instance_group [\n",
        "  {\n",
        "    count: 16\n",
        "    kind: KIND_CPU\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Register the model\n",
        "Register an existing trained model, add description and tags.\n",
        "\n",
        "** Note: ** Under `model_path` there must be a sub-directory named `triton`, which has the structure of a Triton [Model Repository](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html#repository-layout)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Registering model densenet_onnx\ndensenet_onnx Image classification trained on Imagenet Dataset 5\n"
        }
      ],
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model = Model.register(model_path=\"models\", # This points to the local directory to upload.\n",
        "                       model_name=\"densenet_onnx\", # This is the name the model is registered as.\n",
        "                       tags={'area': \"Image classification\", 'type': \"classification\"},\n",
        "                       description=\"Image classification trained on Imagenet Dataset\",\n",
        "                       workspace=ws)\n",
        "\n",
        "print(model.name, model.description, model.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Provision the AKS Cluster\n",
        "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing gpu cluster\n"
        }
      ],
      "source": [
        "from azureml.core.compute import ComputeTarget, AksCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your GPU cluster\n",
        "gpu_cluster_name = \"aks-cluster-1\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
        "    print(\"Found existing gpu cluster\")\n",
        "except ComputeTargetException:\n",
        "    print(\"Creating new gpu-cluster\")\n",
        "    \n",
        "    # Specify the configuration for the new cluster\n",
        "    compute_config = AksCompute.provisioning_configuration(cluster_purpose=AksCompute.ClusterPurpose.DEV_TEST,\n",
        "                                                           agent_count=1,\n",
        "                                                           vm_size=\"Standard_NC6s_v3\")\n",
        "    \n",
        "    # Create the cluster with the specified name and configuration\n",
        "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
        "\n",
        "    # Wait for the cluster to complete, show the output log\n",
        "    gpu_cluster.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy the model as a web service to AKS\n",
        "\n",
        "First create a scoring script\n",
        "\n",
        "** Note: ** Triton server listens to a fixed local port. You may choose to use the Triton Python [client library](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/client_library.html) to talk to it, while keeping the flexibility of pre-/post- processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting score.py\n"
        }
      ],
      "source": [
        "%%writefile score.py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import sys\n",
        "from functools import partial\n",
        "import os\n",
        "import io\n",
        "\n",
        "import tritonhttpclient\n",
        "from tritonclientutils import InferenceServerException\n",
        "\n",
        "from azureml.contrib.services.aml_request import AMLRequest, rawhttp\n",
        "from azureml.contrib.services.aml_response import AMLResponse\n",
        "\n",
        "sys.path.append(os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'models'))\n",
        "from utils import preprocess, postprocess\n",
        "\n",
        "\n",
        "trition_client = None\n",
        "_url = \"localhost:8000\"\n",
        "_model = \"densenet_onnx\"\n",
        "_scaling = \"INCEPTION\"\n",
        "\n",
        "def init():\n",
        "    global triton_client, max_batch_size, input_name, output_name, dtype\n",
        "\n",
        "    triton_client = tritonhttpclient.InferenceServerClient(_url)\n",
        "\n",
        "    max_batch_size = 0\n",
        "    input_name = \"data_0\"\n",
        "    output_name = \"fc6_1\"\n",
        "    dtype = \"FP32\"\n",
        "\n",
        "\n",
        "@rawhttp\n",
        "def run(request):\n",
        "    if request.method == 'POST':\n",
        "        \n",
        "        reqBody = request.get_data(False)\n",
        "        img = Image.open(io.BytesIO(reqBody))\n",
        "        \n",
        "        image_data = preprocess(img, _scaling, dtype)\n",
        "        \n",
        "        input = tritonhttpclient.InferInput(input_name, image_data.shape, dtype)\n",
        "        input.set_data_from_numpy(image_data, binary_data=True)\n",
        "        output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=True, class_count=1)\n",
        "    \n",
        "        res = triton_client.infer(_model,\n",
        "                                [input],\n",
        "                                request_id=\"0\",\n",
        "                                outputs=[output])\n",
        "\n",
        "        result = postprocess(res, output_name, 1, max_batch_size > 0)\n",
        "\n",
        "        return AMLResponse(result, 200)\n",
        "    else:\n",
        "        return AMLResponse(\"bad request\", 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "** Note: ** If you use the Triton Python client, download the Wheels and include them in the `Environment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "triton_client_url = 'https://github.com/NVIDIA/triton-inference-server/releases/download/v2.1.0/v2.1.0_ubuntu1804.clients.tar.gz'\n",
        "\n",
        "client_filename = 'tritonclientutils-2.1.0-py3-none-any.whl'\n",
        "clientutils_filename = 'tritonhttpclient-2.1.0-py3-none-any.whl'\n",
        "target_folder = 'python_client'\n",
        "\n",
        "if not os.path.exists(target_folder):\n",
        "    response = requests.get(triton_client_url)\n",
        "    archive = tarfile.open(fileobj=BytesIO(response.content))\n",
        "    with tempfile.TemporaryDirectory() as temp_folder:\n",
        "        archive.extractall(temp_folder)\n",
        "        os.mkdir(target_folder)\n",
        "        shutil.copy(os.path.join(temp_folder, 'python', client_filename), target_folder)\n",
        "        shutil.copy(os.path.join(temp_folder, 'python', clientutils_filename), target_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create the deployment configuration objects and deploy the model as a webservice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Set the web service configuration (using default here)\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AksWebservice\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.environment import Environment\n",
        "\n",
        "env = Environment(\"deploytocloudenv\")\n",
        "conda_dep = CondaDependencies(\"env.yml\")\n",
        "client_whl_url = Environment.add_private_pip_wheel(workspace=ws, file_path = os.path.join(target_folder, client_filename), exist_ok=True)\n",
        "clientutils_whl_url = Environment.add_private_pip_wheel(workspace=ws, file_path = os.path.join(target_folder, clientutils_filename), exist_ok=True)\n",
        "conda_dep.add_pip_package(client_whl_url)\n",
        "conda_dep.add_pip_package(clientutils_whl_url)\n",
        "env.python.conda_dependencies = conda_dep\n",
        "\n",
        "# Specify the Azure ML Triton base image\n",
        "env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-nvidia-tritonserver20.07-py3'\n",
        "\n",
        "# Optionally specify a worker count to leverage the capability of concurrency and server-side batching from Triton\n",
        "env.environment_variables = {\"WORKER_COUNT\":\"128\"}\n",
        "\n",
        "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n",
        "aks_config = AksWebservice.deploy_configuration(cpu_cores = 30, memory_gb = 40, auth_enabled=False, num_replicas=1, replica_max_concurrent_requests=16)\n",
        "\n",
        "# # Enable token auth and disable (key) auth on the webservice\n",
        "# aks_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 4, gpu_cores = 1, token_auth_enabled=True, auth_enabled=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Running.........\nSucceeded\nAKS service creation operation finished, operation \"Succeeded\"\nHealthy\nCPU times: user 154 ms, sys: 19.2 ms, total: 173 ms\nWall time: 57.6 s\n"
        }
      ],
      "source": [
        "%%time\n",
        "aks_service_name ='densenet-onnx-triton-1'\n",
        "\n",
        "aks_service = Model.deploy(workspace=ws,\n",
        "                           name=aks_service_name,\n",
        "                           models=[model],\n",
        "                           inference_config=inference_config,\n",
        "                           deployment_config=aks_config,\n",
        "                           deployment_target=gpu_cluster)\n",
        "\n",
        "aks_service.wait_for_deployment(show_output = True)\n",
        "print(aks_service.state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test the web service\n",
        "We test the web sevice by passing the test images content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "12.989693 (817) = SPORTS CAR\nCPU times: user 6.77 ms, sys: 714 µs, total: 7.49 ms\nWall time: 365 ms\n"
        }
      ],
      "source": [
        "%%time\n",
        "import requests\n",
        "\n",
        "# if (key) auth is enabled, fetch keys and include in the request\n",
        "#key1, key2 = aks_service.get_keys()\n",
        "\n",
        "#headers = {'Content-Type':'application/octet-stream', 'Authorization': 'Bearer ' + key1}\n",
        "headers = {'Content-Type':'application/octet-stream'}\n",
        "\n",
        "# # if token auth is enabled, fetch token and include in the request\n",
        "# access_token, fetch_after = aks_service.get_token()\n",
        "# headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + access_token}\n",
        "\n",
        "test_sample = open('car.jpg', 'rb').read()\n",
        "resp = requests.post(aks_service.scoring_uri, test_sample, headers=headers)\n",
        "print(resp.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "http://40.78.16.120:80/api/v1/service/densenet-onnx-triton/score\n"
        }
      ],
      "source": [
        "print(aks_service.scoring_uri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clean up\n",
        "Delete the service, image, model and compute target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "aks_service.delete()\n",
        "model.delete()\n",
        "gpu_cluster.delete()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "aks_service.delete()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "vaidyas"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "language": "python",
      "name": "python36964bit49f9d8f83b294f2eb4e5a3f7c26b67fb"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}